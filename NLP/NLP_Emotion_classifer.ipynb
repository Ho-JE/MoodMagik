{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d28e880e",
   "metadata": {
    "id": "d28e880e"
   },
   "outputs": [],
   "source": [
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ab2650",
   "metadata": {
    "id": "40ab2650"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import json\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13a46898",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13a46898",
    "outputId": "2fa25ea5-a2a0-4267-b842-4868296e9cc8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\edcyh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\edcyh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edcyh\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\edcyh\\anaconda3\\lib\\site-packages (0.1.72)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\edcyh\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\edcyh\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\edcyh\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\edcyh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "!pip install contractions \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "from contractions import contractions_dict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize \n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3faf0234",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3faf0234",
    "outputId": "9ae771d1-0f25-4f8c-c8b8-8b7a0c3b7983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  i feel awful about it too because it s my job ...      0\n",
      "1                              im alone i feel awful      0\n",
      "2  ive probably mentioned this before but i reall...      1\n",
      "3           i was feeling a little low few days back      0\n",
      "4  i beleive that i am much more sensitive to oth...      2\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "\n",
    "with open('emotion text/data.jsonl', 'r') as f:\n",
    "    data_str = f.read()\n",
    "    data_str = data_str.replace('\\n', ',')  # add commas between objects\n",
    "    data_str = '[' + data_str[:-1] + ']'   # enclose the entire string in brackets to create a valid JSON array\n",
    "    data_list = json.loads(data_str)\n",
    "\n",
    "# Convert the list of JSON objects into a DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c806745c",
   "metadata": {
    "id": "c806745c"
   },
   "outputs": [],
   "source": [
    "df = df[df['label'].isin([2, 5]) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b663fb92",
   "metadata": {
    "id": "b663fb92"
   },
   "outputs": [],
   "source": [
    "df.replace({4: 2}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "271b72ad",
   "metadata": {
    "id": "271b72ad"
   },
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# Define function to expand contractions\n",
    "def expand_contractions(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "# special_characters removal\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_punctuation_and_splchars(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_word = remove_special_characters(new_word, True)\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "stopword_list= stopwords.words('english')\n",
    "stopword_list.append('im')\n",
    "stopword_list.append('ive')\n",
    "new_words = [\"not\", \"no\", \"never\", \"neither\", \"nor\", \"none\"]\n",
    "for word in new_words:\n",
    "    if word in stopword_list:\n",
    "        stopword_list.remove(word)\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopword_list:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation_and_splchars(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def lemmatize(words):\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return lemmas\n",
    "\n",
    "def normalize_and_lemmaize(input):\n",
    "    sample = denoise_text(input)\n",
    "    sample = remove_special_characters(sample)\n",
    "    words = nltk.word_tokenize(sample)\n",
    "    words = normalize(words)\n",
    "    lemmas = lemmatize(words)\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce2c90eb",
   "metadata": {
    "id": "ce2c90eb"
   },
   "outputs": [],
   "source": [
    "df['clean_data'] = df['text'].map(lambda text: normalize_and_lemmaize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062d5b9",
   "metadata": {
    "id": "5062d5b9"
   },
   "source": [
    "### LTSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "oJW51HVgibT7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "oJW51HVgibT7",
    "outputId": "f8e42508-873f-46c9-bbce-5360af8af1e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>0</td>\n",
       "      <td>feel awful job get position succeed happen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>0</td>\n",
       "      <td>alone feel awful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>1</td>\n",
       "      <td>probably mention really feel proud actually ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>0</td>\n",
       "      <td>feel little low days back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i am one of those people who feels like going ...</td>\n",
       "      <td>1</td>\n",
       "      <td>one people feel like go gym worthwhile hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416804</th>\n",
       "      <td>that was what i felt when i was finally accept...</td>\n",
       "      <td>1</td>\n",
       "      <td>felt finally accept bulgarian conservatorie ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416805</th>\n",
       "      <td>i take every day as it comes i m just focussin...</td>\n",
       "      <td>2</td>\n",
       "      <td>take every day come focus eat better moment no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416806</th>\n",
       "      <td>i just suddenly feel that everything was fake</td>\n",
       "      <td>0</td>\n",
       "      <td>suddenly feel everything fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416807</th>\n",
       "      <td>im feeling more eager than ever to claw back w...</td>\n",
       "      <td>1</td>\n",
       "      <td>feel eager ever claw back go pear shape last w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416808</th>\n",
       "      <td>i give you plenty of attention even when i fee...</td>\n",
       "      <td>0</td>\n",
       "      <td>give plenty attention even feel utterly miserable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367283 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "0       i feel awful about it too because it s my job ...      0   \n",
       "1                                   im alone i feel awful      0   \n",
       "2       ive probably mentioned this before but i reall...      1   \n",
       "3                i was feeling a little low few days back      0   \n",
       "6       i am one of those people who feels like going ...      1   \n",
       "...                                                   ...    ...   \n",
       "416804  that was what i felt when i was finally accept...      1   \n",
       "416805  i take every day as it comes i m just focussin...      2   \n",
       "416806      i just suddenly feel that everything was fake      0   \n",
       "416807  im feeling more eager than ever to claw back w...      1   \n",
       "416808  i give you plenty of attention even when i fee...      0   \n",
       "\n",
       "                                               clean_data  \n",
       "0              feel awful job get position succeed happen  \n",
       "1                                        alone feel awful  \n",
       "2       probably mention really feel proud actually ke...  \n",
       "3                               feel little low days back  \n",
       "6             one people feel like go gym worthwhile hour  \n",
       "...                                                   ...  \n",
       "416804  felt finally accept bulgarian conservatorie ap...  \n",
       "416805  take every day come focus eat better moment no...  \n",
       "416806                      suddenly feel everything fake  \n",
       "416807  feel eager ever claw back go pear shape last w...  \n",
       "416808  give plenty attention even feel utterly miserable  \n",
       "\n",
       "[367283 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e5f5868",
   "metadata": {
    "id": "4e5f5868"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Read in the cleaned text data and labels\n",
    "X = df['clean_data']\n",
    "y = df['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the tokenizer with a maximum vocabulary size of 5000 words\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "\n",
    "# Fit the tokenizer on the training data\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert the text data to numerical sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a maximum length of 100\n",
    "max_seq_length = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_seq_length)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_seq_length)\n",
    "\n",
    "# Convert the label data to a matrix using LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29b9d172",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29b9d172",
    "outputId": "bdb1f36b-78c9-4d85-ce49-aa8710414503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9183/9183 [==============================] - 1094s 119ms/step - loss: 0.0992 - accuracy: 0.9555 - val_loss: 0.0574 - val_accuracy: 0.9707\n",
      "Epoch 2/5\n",
      "9183/9183 [==============================] - 1106s 120ms/step - loss: 0.0523 - accuracy: 0.9721 - val_loss: 0.0573 - val_accuracy: 0.9695\n",
      "Epoch 3/5\n",
      "9183/9183 [==============================] - 1241s 135ms/step - loss: 0.0457 - accuracy: 0.9742 - val_loss: 0.0545 - val_accuracy: 0.9717\n",
      "Epoch 4/5\n",
      "9183/9183 [==============================] - 1135s 124ms/step - loss: 0.0402 - accuracy: 0.9761 - val_loss: 0.0626 - val_accuracy: 0.9709\n",
      "Epoch 5/5\n",
      "9183/9183 [==============================] - 1234s 134ms/step - loss: 0.0367 - accuracy: 0.9774 - val_loss: 0.0633 - val_accuracy: 0.9717\n",
      "2296/2296 [==============================] - 68s 30ms/step - loss: 0.0545 - accuracy: 0.9717\n",
      "Test accuracy: 0.9717385768890381\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, input_length=max_seq_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_pad, y_train, validation_data=(X_test_pad, y_test), epochs=5, batch_size=32, \n",
    "                    callbacks=[EarlyStopping(patience=2, restore_best_weights=True)])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "# Save the model to a file\n",
    "#model.save('NLP_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3KNkfuXWccn",
   "metadata": {
    "id": "d3KNkfuXWccn"
   },
   "outputs": [],
   "source": [
    "model.save('NLP_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22ba17ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 128)          6919040   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              132096    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 4100      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,186,820\n",
      "Trainable params: 7,186,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "new_model = keras.models.load_model('NLP_model.h5')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "oLNcqvX4ETKM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLNcqvX4ETKM",
    "outputId": "bc4176f1-d1fe-4b20-e0e1-d34c0a9e4c5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes model work\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted label: 1\n",
      "\n",
      "0:Sad|1:Happy|2:Fear|3:Anger\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Define the custom text to test\n",
    "custom_text = \"Yes! This model is working\"\n",
    "print(normalize_and_lemmaize(custom_text))\n",
    "\n",
    "# Tokenize the custom text\n",
    "custom_text_seq = tokenizer.texts_to_sequences([normalize_and_lemmaize(custom_text)])\n",
    "\n",
    "# Pad the sequence to the same length as used for the training data\n",
    "custom_text_pad = pad_sequences(custom_text_seq, maxlen=max_seq_length)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = new_model.predict(custom_text_pad)\n",
    "\n",
    "# Convert the predictions to class labels using inverse_transform\n",
    "predicted_labels = lb.inverse_transform(predictions)\n",
    "\n",
    "print('Predicted label:', predicted_labels[0])\n",
    "print(\"\\n0:Sad|1:Happy|2:Fear|3:Anger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ad2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
